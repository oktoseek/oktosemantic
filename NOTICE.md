# NOTICE

This repository serves as a **scientific disclosure and prior-art record** for the **SCTA (Semantic Contrastive Token Approximation)** training method.

---

## Experimental Validation

All reported results were obtained from **real training runs** executed by the author using a private implementation.

The experiments were:
- Conducted on actual hardware (NVIDIA RTX 4070 Laptop GPU, NVIDIA RTX A6000)
- Executed using a private training engine (OktoEngine)
- Reproducible internally using the same implementation
- Validated across multiple vocabulary sizes (50K, 200K, 1M tokens)

---

## Implementation Status

**Source code, training scripts, and optimized kernels are not included** in this repository.

The absence of implementation details is **intentional** and does not imply the absence of a working system.

The method has been:
- ✅ Fully implemented in a private codebase
- ✅ Validated through real training runs
- ✅ Tested on multiple GPUs and vocabulary sizes
- ✅ Demonstrated to achieve the reported performance gains

---

## Repository Purpose

This repository is intended for:

1. **Research Communication**: Formal documentation of the method and results
2. **Authorship Establishment**: Public record of the work and its origin
3. **Prior Art**: Documentation for scientific and patent purposes
4. **Scientific Record**: Evidence of experimental validation

**This repository is NOT intended to provide a reference implementation.**

---

## Scientific Disclosure

This repository contains:

- ✅ Mathematical formulation of the method
- ✅ Theoretical analysis and motivation
- ✅ Complete experimental results
- ✅ Scalability analysis
- ✅ High-resolution figures

This repository does NOT contain:

- ❌ Source code
- ❌ Training scripts
- ❌ Kernel implementations
- ❌ Optimization heuristics
- ❌ Hyperparameter recipes beyond what is needed for interpretation

---

## Citation and Attribution

If you use this work in your research, please cite:

```bibtex
@misc{oliveira2025oktosemantic,
  title={OktoSemantic Training: Semantic Contrastive Token Approximation for Large-Vocabulary Language Models},
  author={Oliveira, Ademir P. de},
  organization={OktoSeek AI},
  year={2025},
  howpublished={\url{https://github.com/oktoseek/oktosemantic}},
  doi={10.5281/zenodo.17931292}
}
```

---

## Contact

**OktoSeek AI**  
Website: [oktoseek.com](https://www.oktoseek.com)  
GitHub: [github.com/oktoseek](https://github.com/oktoseek)

---

**Last Updated**: December 2025  
**Version**: 1.0
